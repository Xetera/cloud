apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: monitoring
data:
  loki.yaml: |-
    auth_enabled: false
    analytics:
      reporting_enabled: false

    limits_config:
      max_query_bytes_read: 64MB
      volume_enabled: true

    server:
      http_listen_port: 3100

    common:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory
      replication_factor: 1
      path_prefix: /tmp/loki

    schema_config:
      configs:
        - from: 2025-02-24
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: index_
            period: 24h

    storage_config:
      aws:
        bucketnames: kubernetes-loki
        endpoint: s3.eu-central-2.wasabisys.com
        region: eu-central-2
        access_key_id: ${S3_LOKI_ACCESS_KEY_ID}
        secret_access_key: ${S3_LOKI_SECRET_ACCESS_KEY}
      tsdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/index_cache

    pattern_ingester:
      enabled: true
      metric_aggregation:
        loki_address: http://loki.monitoring.svc.cluster.local:3100
---
apiVersion: onepassword.com/v1
kind: OnePasswordItem
metadata:
  name: s3-credentials
  namespace: monitoring
spec:
  itemPath: "vaults/k8s-cloud/items/wasabi-s3-credentials"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
        - name: loki
          image: grafana/loki:3.5
          args:
            - --config.expand-env=true
            - --config.file=/etc/loki/loki.yaml
          ports:
            - containerPort: 3100
              protocol: TCP
              name: http
          env:
            - name: S3_LOKI_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: access_key
            - name: S3_LOKI_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: secret_key
          resources:
            requests:
              cpu: 300m
              memory: 500Mi
            limits:
              cpu: 500m
              memory: 1Gi
          volumeMounts:
            - name: loki-data
              mountPath: /data
            - name: loki-config
              mountPath: /etc/loki
      volumes:
        - name: loki-data
          emptyDir: {}
        - name: loki-config
          configMap:
            name: loki-config
---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: monitoring
spec:
  selector:
    app: loki
  ports:
    - port: 3100
      targetPort: 3100
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: loki-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: loki
  podMetricsEndpoints:
    - port: http
---
# only allowing loki to push logs to s3
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "loki-egress"
  namespace: monitoring
spec:
  endpointSelector:
    matchLabels:
      app: loki
  egress:
    - toFQDNs:
        - matchName: "kubernetes-loki.s3.eu-central-2.wasabisys.com"
    - toEndpoints:
        - matchLabels:
            io.kubernetes.pod.namespace: kube-system
            k8s-app: kube-dns
      toPorts:
        - ports:
            - port: "53"
              protocol: ANY
          rules:
            dns:
              - matchPattern: "*"
---
# Alloy
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-config
  labels:
    name: alloy-config
  namespace: monitoring
data:
  config.alloy: |-
    logging {
      level = "debug"
      format = "logfmt"
    }
    discovery.kubernetes "pods" {
      role = "pod"
    }
    discovery.relabel "pods" {
      targets = discovery.kubernetes.pods.targets

      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator     = "/"
        target_label  = "deployment_name"
        action        = "replace"
      }
    }

    loki.source.kubernetes "pods" {
      targets    = discovery.relabel.pods.output
      forward_to = [loki.process.process.receiver]
    }
    loki.process "process" {
      forward_to = [loki.write.loki.receiver]

      stage.drop {
        older_than          = "1h"
        drop_counter_reason = "too old"
      }
      stage.match {
        selector = "{instance=~\".*\"}"
        stage.json {
          expressions = {
            level = "\"level\"",
          }
        }
        stage.labels {
          values = {
            level = "level",
          }
        }
      }
    }
    loki.write "loki" {
      endpoint {
        url = "http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push"
      }
    }

    otelcol.processor.filter "traces_filter" {
      error_mode = "ignore"

      traces {
        span = [
          'IsMatch(attributes["url.path"], "^/healthz?$")',
          'attributes["url.path"] == "/metrics"',
          'IsMatch(attributes["url.path"], "^/favicon")',
        ]
      }
      output {
        traces = [otelcol.exporter.otlphttp.tempo.input]
      }
    }

    otelcol.receiver.otlp "otel" {
      http { }
      grpc { }

      output {
        traces = [otelcol.processor.filter.traces_filter.input]
        metrics = [otelcol.exporter.prometheus.prometheus_exporter.input]
      }
    }

    otelcol.connector.servicegraph "default" {
      dimensions = ["http.method", "http.target"]
      output {
        metrics = [otelcol.exporter.otlphttp.tempo.input]
      }
    }

    // otelcol.processor.deltatocumulative "LABEL" {
    //   output {
    //     metrics = [otelcol.exporter.prometheus.prometheus_exporter.input]
    //   }
    // }

    otelcol.exporter.prometheus "prometheus_exporter" {
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }

    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus-stack-kube-prom-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
      }
    }

    otelcol.processor.k8sattributes "default" {
      extract {
        metadata = [
          "k8s.namespace.name",
          "k8s.pod.name",
          "k8s.container.name",
        ]
      }

      output {
        traces = [otelcol.exporter.otlphttp.tempo.input]
      }
    }

    otelcol.exporter.otlphttp "tempo" {
      client {
        endpoint = "http://tempo.monitoring.svc.cluster.local:4318"
      }
      encoding  = "json"
    }

    loki.echo "example" { }
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alloy-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alloy
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alloy
  template:
    metadata:
      labels:
        app: alloy
    spec:
      serviceAccountName: alloy
      containers:
        - name: alloy
          image: grafana/alloy:v1.9.1
          ports:
            - containerPort: 12345
              name: http
            - containerPort: 4318
            - containerPort: 4317
          args:
            - run
            - /etc/alloy/config.alloy
            - --server.http.listen-addr=0.0.0.0:12345
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 500m
              memory: 1Gi
          volumeMounts:
            - name: alloy-config
              mountPath: /etc/alloy
            - name: alloy-data
              mountPath: /var/lib/alloy/data
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 12345
            initialDelaySeconds: 10
            periodSeconds: 10

      volumes:
        - name: alloy-config
          configMap:
            name: alloy-config
        - name: alloy-data
          persistentVolumeClaim:
            claimName: alloy-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: alloy
  namespace: monitoring
spec:
  selector:
    app: alloy
  ports:
    - name: http
      protocol: TCP
      port: 12345
      targetPort: 12345
    - name: grpc-over-http
      protocol: TCP
      port: 4318
      targetPort: 4318
    - name: grpc
      protocol: TCP
      port: 4317
      targetPort: 4317
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: alloy-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: alloy
  podMetricsEndpoints:
    - port: http
